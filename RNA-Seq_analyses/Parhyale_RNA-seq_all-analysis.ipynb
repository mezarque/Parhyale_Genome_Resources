{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNA-Seq Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To convert this to a Python script, use:\n",
    "#jupyter nbconvert --to script Omni-ATAC_all-analysis.ipynb\n",
    "\n",
    "#To run on a SLURM scheduler or other shared computing system, you may need to load modules.\n",
    "#For Savio, utilize the script ATAC-Seq_phaw5.0.sh\n",
    "\n",
    "#To upload this to SLURM, along with other necessary files, use\n",
    "#scp Omni-ATAC_all-analysis.py Omni-ATAC_all-analysis.sh dasun@dtn.brc.berkeley.edu:/global/scratch/dasun/Omni-ATAC-Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuration of Python script\n",
    "import subprocess\n",
    "import os\n",
    "import os.path\n",
    "import fnmatch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Trimming with trim-galore\n",
    "\n",
    "This section of the script trims the Illumina adapters off the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "### Fill in this information before running this section ####\n",
    "\n",
    "#Decide if you want to run this section\n",
    "run_read_trimming = False\n",
    "\n",
    "#Write a base pattern for your raw fastq file names\n",
    "#This pattern should occur in all of your file names\n",
    "base_pattern = 'S*_R1_001.fastq.gz'\n",
    "\n",
    "#############################################################\n",
    "#############################################################\n",
    "\n",
    "#checks if you want to trim your reads\n",
    "if run_read_trimming == True:\n",
    "\n",
    "    #Scan through all of the files in the folder specified within the single quotes in the line below\n",
    "    #for each file listed in a sorted list of the directory's contents\n",
    "    for file in sorted(os.listdir('.')):\n",
    "        \n",
    "        #determines if the file is a .fastq.gz file\n",
    "        if fnmatch.fnmatch(file, base_pattern):\n",
    "            \n",
    "            #gets the name of that file\n",
    "            one = file\n",
    "            \n",
    "            #simulates the name of the other read of that file\n",
    "            two = file.replace('R1_001','R2_001')\n",
    "            \n",
    "            #checks if there is not an appropriate pair for the file, if so, then exits and returns an error\n",
    "            if not os.path.exists(two):\n",
    "                raise Exception('A matching paired file for', file, 'was not found')\n",
    "                \n",
    "            #sends progress message to stdout\n",
    "            !echo 'Performing trim-galore on' $one 'and' $two\n",
    "            \n",
    "            #performs trim_galore on the pair of reads identified\n",
    "            #removing automatically detected adapters and performing FastQC\n",
    "            !trim_galore --illumina --fastqc --paired --stringency 5 --gzip --retain_unpaired $one $two\n",
    "\n",
    "    #Moves the .html and .zip files into a folder called \"FASTQC\"\n",
    "    ![ -d \"FASTQC\" ] && echo 'FASTQC folder already exists' || echo 'Making FASTQC folder'; mkdir FASTQC\n",
    "    !mv *.zip FASTQC\n",
    "    !mv *.html FASTQC\n",
    "    !mv *.txt FASTQC\n",
    "\n",
    "    #Make the directory \"untrimmed\" if it does not already exist\n",
    "    ![ -d \"untrimmed\" ] && echo 'untrimmed directrory already exists' || echo 'Making untrimmed folder'; mkdir untrimmed\n",
    "    !mv *.fastq.gz untrimmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Trimming with trim-galore (round 2)\n",
    "\n",
    "After FASTQC, we observed an additional adapter sequence left over from the TaKaRa SMART-Seq kit:\n",
    "\n",
    "AAGCAGTGGTATCAACGCAGAGTAC\n",
    "\n",
    "To remove this, and perform quality trimming, we perform a second round of trim-galore trimming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "### Fill in this information before running this section ####\n",
    "\n",
    "#Decide if you want to run this section\n",
    "run_read_trimming_rd2 = False\n",
    "\n",
    "#Write a base pattern for your raw fastq file names\n",
    "#This pattern should occur in all of your file names\n",
    "base_pattern = 'S*_R1_001_val_1.fq.gz'\n",
    "\n",
    "#############################################################\n",
    "#############################################################\n",
    "\n",
    "#checks if you want to trim your reads\n",
    "if run_read_trimming_rd2 == True:\n",
    "\n",
    "    #Scan through all of the files in the folder specified within the single quotes in the line below\n",
    "    #for each file listed in a sorted list of the directory's contents\n",
    "    for file in sorted(os.listdir('.')):\n",
    "        \n",
    "        #determines if the file is a .fastq.gz file\n",
    "        if fnmatch.fnmatch(file, base_pattern):\n",
    "            \n",
    "            #gets the name of that file\n",
    "            one = file\n",
    "            \n",
    "            #simulates the name of the other read of that file\n",
    "            two = file.replace('R1', 'R2').replace('val_1', 'val_2')\n",
    "            \n",
    "            #generates new names for the files\n",
    "            new_one = one.replace('val', 'rd1')\n",
    "            new_two = two.replace('val', 'rd1')\n",
    "            \n",
    "            #checks if there is not an appropriate pair for the file, if so, then exits and returns an error\n",
    "            if not os.path.exists(two):\n",
    "                raise Exception('A matching paired file for', file, 'was not found')\n",
    "            \n",
    "            #renames the original files\n",
    "            !echo 'Renaming ' $one ' to ' $new_one\n",
    "            !mv $one $new_one\n",
    "            !echo 'Renaming ' $two ' to ' $new_two\n",
    "            !mv $two $new_two\n",
    "\n",
    "            #sends progress message to stdout\n",
    "            !echo 'Performing trim-galore round 2 on' $new_one 'and' $new_two\n",
    "            \n",
    "            #performs trim_galore on the pair of reads identified\n",
    "            #removing automatically detected adapters and performing FastQC again\n",
    "            !trim_galore -q 20 --fastqc --paired -a AAGCAGTGGTATCAACGCAGAGTAC --stringency 5 --gzip --retain_unpaired $new_one $new_two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trinity RNA-Seq de novo Assembly\n",
    "Use the code below to assemble via Trinity RNA-Seq.\n",
    "You can specify to run either *de novo* assembly, genome-guided assembly, or both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "### Fill in this information before running this section ####\n",
    "\n",
    "#Decide if you want to make a sample file\n",
    "run_make_Trinity_samplefile = False\n",
    "run_Trinity_denovo = False\n",
    "\n",
    "#Write a dictionary with prefixes and conditions\n",
    "samples_file_name = '20210429_samples_file.txt'\n",
    "base_pattern_2 = '*_val_1.fq.gz'\n",
    "\n",
    "#############################################################\n",
    "#############################################################\n",
    "\n",
    "#checks if you want to generate a sample file\n",
    "if run_make_Trinity_samplefile == True:\n",
    "\n",
    "    #make empty dataframe for handling data\n",
    "    dataframe = pd.DataFrame(columns=['Condition', 'Replicate', 'FwdSeq', 'RevSeq'])\n",
    "\n",
    "    #search through current directory\n",
    "    for file in sorted(os.listdir('.')):\n",
    "        \n",
    "        #check if it's read1\n",
    "        if fnmatch.fnmatch(file, base_pattern_2):\n",
    "            \n",
    "            prefix = file.split('_')[0]\n",
    "            \n",
    "            #simulate the name of read2\n",
    "            read1, read2 = file, file.replace('R1', 'R2').replace('val_1', 'val_2').replace('rd1_1', 'rd1_2')\n",
    "            unpaired1 = read1.replace('val', 'unpaired')\n",
    "            unpaired2 = read2.replace('val', 'unpaired')\n",
    "            \n",
    "            #check if the other read exists in the current directory\n",
    "            #if not, move on to examining next file\n",
    "            if not os.path.exists(read2):\n",
    "                raise Exception('A matching paired file for', read1, 'was not found')\n",
    "                continue\n",
    "            if not os.path.exists(unpaired1):\n",
    "                raise Exception('A unpaired read1 file for', unpaired1, 'was not found')\n",
    "                continue\n",
    "            if not os.path.exists(unpaired2):\n",
    "                raise Exception('A unpaired read2 file for', unpaired2, 'was not found')\n",
    "                continue\n",
    "            \n",
    "            #generate \"full\" datasets for left and right read pairs so Trinity will handle them\n",
    "            newname1 = read1.replace('val_1', 'all_1')\n",
    "            newname2 = read2.replace('val_2', 'all_2')\n",
    "            \n",
    "            !cat $read1 $unpaired1 > $newname1\n",
    "            !cat $read2 $unpaired2 > $newname2\n",
    "                \n",
    "            #tell the user that a pair has been discovered\n",
    "            print('Adding rows for', newname1, newname2)\n",
    "            \n",
    "            #append data to the dataframe\n",
    "            dataframe = dataframe.append({'Condition': prefix[0:3], \n",
    "                                        'Replicate': prefix,\n",
    "                                        'FwdSeq': newname1, \n",
    "                                        'RevSeq': newname2}, ignore_index = True)\n",
    "\n",
    "    #write sample file\n",
    "    print('Writing sample file to', samples_file_name)\n",
    "    dataframe.to_csv(samples_file_name, sep='\\t', index = False, header = False)\n",
    "\n",
    "if run_Trinity_denovo == True:\n",
    "    !Trinity --seqType fq --max_memory 1500G --samples_file $samples_file_name --CPU 32 --output trinity_out_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trinity RNA-Seq Genome Guided Assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "### Fill in this information before running this section ####\n",
    "\n",
    "#Decide if you want to make a sample file\n",
    "run_bowtie2_align = False\n",
    "run_combine_bams = False\n",
    "run_Trinity_genomeguided = False\n",
    "run_insilico_mapping = False\n",
    "run_Trinity_gg_insilico = False\n",
    "run_Trinity_gg_hisat2 = False\n",
    "\n",
    "#set q score cutoff for bowtie2 alignment\n",
    "qscore = 10\n",
    "\n",
    "#set the maximum intron size for genome-guided assembly\n",
    "max_intron = 300000\n",
    "\n",
    "#Set the names of your .fasta file\n",
    "genome_file = 'phaw_5.0.fa'\n",
    "#Container for the chromsizes variable\n",
    "bt2lib = genome_file.replace('.fa', '.bt2lib')\n",
    "bt2libmarker = genome_file.replace('.fa', '.bt_marker.txt')\n",
    "bt2marker_exists = True\n",
    "chromsizes = genome_file.replace('.fa', '.chrom.sizes')\n",
    "subfinalfile = 'Sall_RNA_unsorted.bam'\n",
    "finalfile = 'Sall_RNA_sorted.bam'\n",
    "\n",
    "#############################################################\n",
    "#############################################################\n",
    "\n",
    "#checks if you want to trim your reads\n",
    "if run_bowtie2_align == True:\n",
    "\n",
    "    #check if the bt2 marker has already been flagged\n",
    "    !printf 'checking if index file has already been generated for '$genome_file'\\n'\n",
    "    for file in sorted(os.listdir('.')):\n",
    "        if fnmatch.fnmatch(file, bt2libmarker):\n",
    "            bt2marker_exists = True\n",
    "            !printf 'genome index file already exists for '$genome_file'\\n'\n",
    "\n",
    "    #search through current directory\n",
    "    for file in sorted(os.listdir('.')):\n",
    "        #if the genome file is found and there is no bt2marker_exists flag\n",
    "        if fnmatch.fnmatch(file, genome_file) and bt2marker_exists == False:\n",
    "            #check for files in the current directory\n",
    "            !printf 'no index file exists for '$genome_file'\\n'\n",
    "            !printf 'generating bt2 index for '$genome_file'\\n'\n",
    "            bt2_output = genome_file.replace('.fa', '_bowtie2-build-output.txt')\n",
    "            !bowtie2-build -f --threads 12 $genome_file $bt2lib > $bt2_output\n",
    "        \n",
    "            #generate chrom.sizes file\n",
    "            fainame = genome_file + '.fai'\n",
    "            !samtools faidx $genome_file\n",
    "            !cut -f1,2 $fainame > $chromsizes\n",
    "\n",
    "            #make a marker file to indicate that the bowtie2 library has already been built\n",
    "            !touch $bt2libmarker\n",
    "\n",
    "    #check for files in the current directory\n",
    "    for file in sorted(os.listdir('.')):\n",
    "    \n",
    "        #initialize and/or blank variables\n",
    "        one, two, unone, untwo = '', '', '', ''\n",
    "    \n",
    "        #determine the file name and its partners, if it is R1 trimmed read\n",
    "        if fnmatch.fnmatch(file,'S*rd1_1_val_1.fq.gz'):\n",
    "            one = file\n",
    "            two = file.replace('R1', 'R2').replace('val_1', 'val_2').replace('rd1_1', 'rd1_2')\n",
    "            unone = one.replace('val', 'unpaired')\n",
    "            untwo = two.replace('val', 'unpaired')\n",
    "            print('Starting bowtie2 alignment on ', one, two, unone, untwo)\n",
    "        else: continue\n",
    "        \n",
    "        #get prefix of samples\n",
    "        prefix = one.split('_')[0]\n",
    "        \n",
    "        #name sam file\n",
    "        samname = prefix + '_RNA.sam'\n",
    "    \n",
    "        #run bowtie2\n",
    "        !printf 'running bowtie2 on '$prefix'\\n'\n",
    "        !bowtie2 --local --very-sensitive-local --threads 40 --time -x $bt2lib -1 $one -2 $two -U $unone,$untwo -S $samname\n",
    "\n",
    "        #name bam files\n",
    "        bampref = samname.replace('.sam','_sorted')\n",
    "        bamname = samname.replace('.sam','_q' + str(qscore) + '.bam')\n",
    "        !printf 'running samtools sort on '$prefix'\\n'\n",
    "        !samtools view -bS -q $qscore $samname | samtools sort -T $bampref -o $bamname\n",
    "\n",
    "if run_combine_bams == True:\n",
    "    \n",
    "    #initialize container for all bam file names\n",
    "    bam_list = ''\n",
    "    bam_num = 0\n",
    "    \n",
    "    for file in sorted(os.listdir('.')):\n",
    "        if fnmatch.fnmatch(file,'S*_RNA_q10.bam'):\n",
    "            if bam_num == 0:\n",
    "                bam_list = file\n",
    "                bam_num = bam_num + 1\n",
    "                !printf 'Found file '$file' and added to list at position '$bam_num'\\n'\n",
    "            else:\n",
    "                bam_list = bam_list + ' ' + file\n",
    "                bam_num = bam_num + 1\n",
    "                !printf 'Found file '$file' and added to list at position '$bam_num'\\n'\n",
    "        else: continue\n",
    "    \n",
    "    !printf 'Final list of bams is: $bam_list\\n'\n",
    "    \n",
    "    #merge all bam files into one\n",
    "    finalbampref = finalfile.split('.')[0]\n",
    "    !printf 'Merging $bam_list into $subfinalfile\\n'\n",
    "    !samtools merge $subfinalfile $bam_list\n",
    "    !printf 'Sorting $subfinalfile into $finalfile\\n'\n",
    "    !samtools sort -T $finalbampref -o $finalfile $subfinalfile\n",
    "\n",
    "if run_Trinity_genomeguided == True:\n",
    "    !Trinity --genome_guided_bam $finalfile --genome_guided_max_intron $max_intron --max_memory 1500G --CPU 32 --bflyHeapSpaceMax 20G --bflyHeapSpaceInit 4G --bflyCalculateCPU --output trinity_out_dir_GG\n",
    "\n",
    "if run_insilico_mapping == True:\n",
    "    r1 = 'S13A1_S84_R1_001_rd1_1_all_1.fq.gz_ext_all_reads.normalized_K25_C50_pctSD10000.fq'\n",
    "    r2 = 'S13A1_S84_R2_001_rd1_2_all_2.fq.gz_ext_all_reads.normalized_K25_C50_pctSD10000.fq'\n",
    "    samname = 'Phaw_RNA_insilico.sam'\n",
    "    !printf 'running bowtie2 on '$r1', '$r2'\\n'\n",
    "    !bowtie2 --local --very-sensitive-local --threads 40 --time -x $bt2lib -1 $r1 -2 $r2 -S $samname\n",
    "    bampref = samname.replace('.sam','_sorted')\n",
    "    bamname = samname.replace('.sam','_q' + str(qscore) + '.bam')\n",
    "    !printf 'running samtools sort on '$prefix'\\n'\n",
    "    !samtools view -bS -q $qscore $samname | samtools sort -T $bampref -o $bamname\n",
    "\n",
    "if run_Trinity_gg_insilico == True:\n",
    "    finalfile2 = 'Phaw_RNA_insilico_q10.bam'\n",
    "    !Trinity --genome_guided_bam $finalfile2 --genome_guided_max_intron $max_intron --max_memory 1500G --CPU 32 --output trinity_out_dir_GG_insilico\n",
    "\n",
    "if run_Trinity_gg_hisat2 == True:\n",
    "    subfinalfile = 'Phaw_RNA_hisat2.cleaned.bam'\n",
    "    finalbampref = subfinalfile.replace('.bam', '')\n",
    "    finalfile = subfinalfile.replace('.bam', '.sorted.bam')\n",
    "    !samtools sort -T $finalbampref -o $finalfile $subfinalfile\n",
    "    !Trinity --genome_guided_bam $finalfile --genome_guided_max_intron $max_intron --max_memory 1500G --CPU 32 --output trinity_out_dir_GG_hisat2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make individual transcriptomes per developmental stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "### Fill in this information before running this section ####\n",
    "\n",
    "#Decide if you want to make a sample file\n",
    "run_make_Trinity_samplefile_singles = False\n",
    "run_Trinity_denovo_singles = False\n",
    "\n",
    "#Write a dictionary with prefixes and conditions\n",
    "samples_file_basename = '20210429_*_samples_file.txt'\n",
    "base_pattern_3 = '*_all_1.fq.gz'\n",
    "\n",
    "#############################################################\n",
    "#############################################################\n",
    "\n",
    "#set global variables\n",
    "file_dict = {}\n",
    "stages = ['S13', 'S19', 'S21', 'S23']\n",
    "\n",
    "#checks if you want to generate a sample file\n",
    "if run_make_Trinity_samplefile_singles == True:\n",
    "\n",
    "    #make empty dataframe for handling data\n",
    "    for stage in stages:\n",
    "        dataframe = pd.DataFrame(columns=['Condition', 'Replicate', 'FwdSeq', 'RevSeq'])\n",
    "        file_dict[stage] = dataframe\n",
    "        \n",
    "    #search through current directory\n",
    "    for file in sorted(os.listdir('.')):\n",
    "        \n",
    "        #check if it's read1\n",
    "        if fnmatch.fnmatch(file, base_pattern_3):\n",
    "            \n",
    "            prefix = file.split('_')[0]\n",
    "            \n",
    "            #simulate the name of read2\n",
    "            read1, read2 = file, file.replace('R1', 'R2').replace('all_1', 'all_2').replace('rd1_1', 'rd1_2')\n",
    "                \n",
    "            #tell the user that a pair has been discovered\n",
    "            !printf 'Adding rows for $read1 and $read2'\n",
    "            \n",
    "            #append data to the dataframe\n",
    "            file_dict[prefix[0:3]] = file_dict[prefix[0:3]].append({'Condition': prefix[0:3], \n",
    "                                        'Replicate': prefix,\n",
    "                                        'FwdSeq': read1,\n",
    "                                        'RevSeq': read2}, ignore_index = True)\n",
    "\n",
    "    #write sample file\n",
    "    for dict_stage in file_dict:\n",
    "        sample_file_forstage = samples_file_basename.replace('_*_', '_' + dict_stage + '_')\n",
    "        !printf 'Writing sample file to $sample_file_forstage'\n",
    "        file_dict[dict_stage].to_csv(sample_file_forstage, sep='\\t', index = False, header = False)\n",
    "\n",
    "if run_Trinity_denovo_singles == True:\n",
    "    samples_file_list = []\n",
    "    counter = 0\n",
    "\n",
    "    for file in sorted(os.listdir('.')):\n",
    "        #check if it's read1\n",
    "        if fnmatch.fnmatch(file, samples_file_basename):\n",
    "            if counter == 0:\n",
    "                samples_file_list = [file]\n",
    "                counter = counter + 1\n",
    "            else:\n",
    "                samples_file_list = samples_file_list + [file]\n",
    "                counter = counter + 1\n",
    "    \n",
    "    for file in samples_file_list:\n",
    "        output_dir = 'trinity_out_dir_' + file.split('_')[1]\n",
    "        !printf 'Generating Trinity run for $samples_file_list'\n",
    "        !Trinity --seqType fq --max_memory 1500G --samples_file $file --CPU 32 --output $output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S13A1/abundance.tsv S13B1/abundance.tsv S13C1/abundance.tsv S15A1/abundance.tsv S15B1/abundance.tsv S15C1/abundance.tsv S21A/abundance.tsv S21B/abundance.tsv S21C/abundance.tsv S23A/abundance.tsv S23B/abundance.tsv S23C/abundance.tsv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#############################################################\n",
    "### Fill in this information before running this section ####\n",
    "\n",
    "#Decide if you want to make a sample file\n",
    "run_kallisto = False\n",
    "run_kallisto_matrix = True\n",
    "\n",
    "#Write a dictionary with prefixes and conditions\n",
    "samples_file = '20210429_samples_file.txt'\n",
    "transcript_file = 'DAS.mikado.loci.fasta'\n",
    "gene_trans_map_file = 'DAS.mikado.loci.fasta.gene_trans_map'\n",
    "output_dir = transcript_file + '_run'\n",
    "\n",
    "stages = ['S13A1', 'S13B1', 'S13C1', 'S19A1', 'S19B1', 'S19C1', 'S21A', 'S21B', 'S21C', 'S23A', 'S23B', 'S23C']\n",
    "basedirs_list = [i + '/abundance.tsv' for i in stages]\n",
    "basedirs = ' '.join(basedirs_list)\n",
    "\n",
    "#############################################################\n",
    "#############################################################\n",
    "\n",
    "#set global variables\n",
    "if run_kallisto == True:\n",
    "    !/clusterfs/vector/home/groups/software/sl-7.x86_64/modules/trinity/2.5.1/util/align_and_estimate_abundance.pl --transcripts {transcript_file} --samples_file {samples_file} --seqType fq --est_method kallisto --output_dir {output_dir} --gene_trans_map {gene_trans_map_file} --prep_reference --thread_count 12\n",
    "    \n",
    "if run_kallisto_matrix == True:\n",
    "    !/clusterfs/vector/home/groups/software/sl-7.x86_64/modules/trinity/2.5.1/util/abundance_estimates_to_matrix.pl --est_method kallisto --gene_trans_map {gene_trans_map_file} --name_sample_by_basedir {basedirs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
