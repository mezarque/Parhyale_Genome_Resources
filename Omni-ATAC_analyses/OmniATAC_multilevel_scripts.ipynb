{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Identify best contig in Parhyale genome using blast_to_gff.py\n",
    "\n",
    "You can get blast_to_gff.py from [here](https://github.com/alvaralmstedt/py_scripts). </br>\n",
    "You can get blast_to_gff_wrapper.sh from [here](https://github.com/alvaralmstedt/shell_scripts).\n",
    "\n",
    "1. Set the transcript FASTA file of interest to blastn to the Parhyale genome.\n",
    "2. Load the output into IGV and navigate to the region of interest using the 'IGV_address' field in the table that the cell below prints.\n",
    "3. Identify a window around the gene including all 3' and 5' regions surrounding the gene.\n",
    "4. Log this window into the [phaw_5.0 Genomic Addresses document](https://docs.google.com/spreadsheets/d/1rFzF0x8vpltd60TerYOCvBwXUBAFvdwSScArHvy_zD0/edit?usp=sharing).\n",
    "5. Use the IGV coordinates for this window in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blasttype = 'nucl'\n",
    "gene_name = 'CRY2'\n",
    "transcript_name = 'Par-haw_CRY2.fasta'\n",
    "\n",
    "if transcript_name != '' and blasttype == 'nucl':\n",
    "    transcript_loc = '~/Labwork/Bioinformatics/Transcripts/'\n",
    "    transcript = transcript_loc + transcript_name\n",
    "elif transcript_name != '' and blasttype == 'prot':\n",
    "    transcript_loc = '~/Labwork/Bioinformatics/Proteins/'\n",
    "    transcript = transcript_loc + transcript_name\n",
    "elif blasttype == 'prot':\n",
    "    transcript_name = gene_name + '_protein.fasta'\n",
    "    transcript_loc = '~/Labwork/Bioinformatics/Proteins/'\n",
    "    transcript = transcript_loc + transcript_name\n",
    "elif blasttype == 'nucl':\n",
    "    transcript_name = gene_name + '.fasta'\n",
    "    transcript_loc = '~/Labwork/Bioinformatics/Transcripts/'\n",
    "    transcript = transcript_loc + transcript_name\n",
    "\n",
    "#transcript = '~/Labwork/Bioinformatics/ContigsandScaffolds/Par-haw_Sp69-p2-old.fasta'\n",
    "\n",
    "genome_blastdb = '~/Labwork/Bioinformatics/GenomeSequences/Phaw_5.0_Annotation/genome/phaw_5.0.blastdb'\n",
    "script_loc = '~/Labwork/Bioinformatics/Scripts/shell_scripts-master/blast_to_gff_wrapper.sh'\n",
    "\n",
    "if blasttype == 'prot':\n",
    "    output_file = transcript.replace('Proteins', 'BLASTResults').replace('.fasta', '_vs_phaw5.0.blastn')\n",
    "    !$script_loc -q $transcript -d $genome_blastdb -p tblastn -t 4 -l -o $output_file\n",
    "elif blasttype == 'nucl':\n",
    "    output_file = transcript.replace('Transcripts', 'BLASTResults').replace('.fasta', '_vs_phaw5.0.blastn')\n",
    "    !$script_loc -q $transcript -d $genome_blastdb -p blastn -t 4 -l -o $output_file\n",
    "\n",
    "gff_file = output_file + '.gff'\n",
    "\n",
    "output = pd.read_csv(gff_file, sep = '\\t', header = None, skiprows = 1)\n",
    "output['IGV_address'] = output[0] + ':' + output[3].astype(str) + '-' + output[4].astype(str)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract FASTA from Parhyale genome\n",
    "\n",
    "Running the cell below allows you to extract FASTA sequence files from your genome of choice based on genome coordinates from IGV.\n",
    "\n",
    "Be sure to set the variables for the run in the section before the hashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input the IGV address of the region you want to extract\n",
    "#The script tolerates commas and removes them\n",
    "igv_address = 'phaw_50.283815c:29,301,115-29,375,053'\n",
    "nickname = 'Phaw-BmaI'\n",
    "\n",
    "#set whether or not you want to extract the fasta file for the whole region\n",
    "get_region = True\n",
    "\n",
    "#set the location of the fasta file you want to extract from\n",
    "fasta_file = '~/Labwork/Bioinformatics/GenomeSequences/Phaw_5.0_Annotation/genome/phaw_5.0.fa'\n",
    "#set the destination directory for your output fasta files\n",
    "output_destination = '~/Labwork/Bioinformatics/ContigsandScaffolds/'\n",
    "\n",
    "#set whether or not you want to extract fasta files of the peaks of that region\n",
    "get_peaks = False\n",
    "peak_expansion = 500\n",
    "\n",
    "#set the location of the peak region file you want to compare to\n",
    "peaks_file = '~/Labwork/Bioinformatics/Omni-ATAC-Seq/OmniATAC_bothruns_peaks/OmniATAC_bothruns_q005_allpeaks.igv_new.bed'\n",
    "\n",
    "####################################################\n",
    "####################################################\n",
    "\n",
    "#Decompose the IGV address into component parts\n",
    "contig = igv_address.split(':')[0]\n",
    "start = igv_address.split(':')[1].split('-')[0].replace(',', '')\n",
    "end = igv_address.split(':')[1].split('-')[1].replace(',', '')\n",
    "print('genomic region of interest is:', contig, start, end, '\\n')\n",
    "\n",
    "#Run this block to get the fasta file from your region of interest\n",
    "if get_region == True:\n",
    "    region = contig + '\\t' + start + '\\t' + end\n",
    "    region_name = contig + '_' + start + '_' + end \n",
    "    with open('region.bed', 'w+') as f:\n",
    "        f.write(region)\n",
    "    region_fasta = output_destination + nickname + '_' + region_name + '.fasta'\n",
    "    region_bed = 'region.bed'\n",
    "    print('getting fasta file of coordinate', igv_address, 'and saving to:\\n', region_fasta, '\\n')\n",
    "    !/usr/local/bin/bedtools getfasta -fi $fasta_file -bed $region_bed -fo $region_fasta\n",
    "\n",
    "#Run this block to get the fasta file of peaks overlapping your region of interest\n",
    "if get_peaks == True:\n",
    "    region = contig + '\\t' + start + '\\t' + end\n",
    "    region_name = contig + '_' + start + '_' + end + '_peaks'\n",
    "    with open('region.bed', 'w+') as f:\n",
    "        f.write(region)\n",
    "    region_bed = 'region.bed'\n",
    "    region_peaks_bed = 'region_peaks.bed'\n",
    "    !bedtools intersect -wa -a $peaks_file -b $region_bed > $region_peaks_bed\n",
    "    peaks = pd.read_csv(region_peaks_bed, sep = '\\t', header = None)\n",
    "    peaks[1] = peaks[1] - peak_expansion\n",
    "    peaks[2] = peaks[2] + peak_expansion\n",
    "    peaks.to_csv(region_peaks_bed, sep = '\\t', header = None, index = None)\n",
    "    region_peaks_fasta = output_destination + nickname + '_' + region_name + '.fasta'\n",
    "    print('getting fasta file of expanded peaks within coordinate', igv_address, 'and saving to:\\n', region_peaks_fasta, '\\n')\n",
    "    !bedtools getfasta -fi $fasta_file -bed $region_peaks_bed -fo $region_peaks_fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify best contig in Hyalella genome using blast_to_gff.py\n",
    "1. Set the transcript FASTA file of interest to blastn to the Hyalella genome.\n",
    "2. Load the output into IGV and navigate to the region of interest using the 'IGV_address' field in the table that the cell below prints.\n",
    "3. Double-check the address using the online [i5k BLAST webapp](https://i5k.nal.usda.gov/webapp/blast/). If the i5k differs from the BLAST from this script, go with the i5k address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blasttype = 'prot'\n",
    "gene_name = 'dll-e'\n",
    "transcript_name = 'Par-haw_' + gene_name + '.fasta'\n",
    "protein_name = 'Par-haw_' + gene_name + '_protein.fasta'\n",
    "\n",
    "transcript_loc = '~/Labwork/Bioinformatics/Transcripts/'\n",
    "protein_loc = '~/Labwork/Bioinformatics/Proteins/'\n",
    "script_loc = '~/Labwork/Bioinformatics/Scripts/shell_scripts-master/blast_to_gff_wrapper.sh'\n",
    "transcript = ''\n",
    "genome_blastdb = '~/Labwork/Bioinformatics/GenomeSequences/Hya_azt/Hazt_2.0.1_genomic.blastdb'\n",
    "\n",
    "if blasttype == 'nucl':\n",
    "    transcript = transcript_loc + transcript_name\n",
    "    output_file = transcript.replace('Transcripts', 'BLASTResults').replace('.fasta', '_vs_Hazt2.0.1.blastn')\n",
    "    !$script_loc -q $transcript -d $genome_blastdb -p blastn -t 4 -l -o $output_file\n",
    "elif blasttype == 'prot':\n",
    "    transcript = protein_loc + protein_name\n",
    "    output_file = transcript.replace('Proteins', 'BLASTResults').replace('.fasta', '_vs_Hazt2.0.1.blastn')\n",
    "    !$script_loc -q $transcript -d $genome_blastdb -p tblastn -t 4 -l -o $output_file\n",
    "\n",
    "gff_file = output_file + '.gff'\n",
    "\n",
    "output = pd.read_csv(gff_file, sep = '\\t', header = None, skiprows = 1)\n",
    "output['IGV_address'] = output[0] + ':' + output[3].astype(str) + '-' + output[4].astype(str)\n",
    "display(output.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Hyalella BLAST .gffs together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of filenames\n",
    "directory = '/users/dennis/Labwork/Bioinformatics/BLASTResults/'\n",
    "filenames = [file for file in os.listdir(directory) if 'vs_Hazt2.0.1.blastn.gff' in file]\n",
    "\n",
    "# Open file3 in write mode \n",
    "with open(directory + 'Hazt_BLAST_results.gff', 'w+') as outfile: \n",
    "  \n",
    "    # Iterate through list \n",
    "    for name in filenames: \n",
    "  \n",
    "        # Open each file in read mode \n",
    "        with open(directory + name) as infile: \n",
    "  \n",
    "            # read the data from file1 and \n",
    "            # file2 and write it in file3 \n",
    "            outfile.write(infile.read()) \n",
    "  \n",
    "        # Add '\\n' to enter data of file2 \n",
    "        # from next line \n",
    "        outfile.write(\"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract FASTA from Hyalella genome\n",
    "\n",
    "Running the cell below allows you to extract FASTA sequence files from your genome of choice based on genome coordinates from IGV.\n",
    "\n",
    "Be sure to set the variables for the run in the section before the hashes.\n",
    "\n",
    "You can also validate your BLAST result using the [online BLAST for Hyalella at i5k](https://i5k.nal.usda.gov/webapp/blast/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input the IGV address of the region you want to extract\n",
    "igv_address = 'JQDR03001188.1:1-207,259'\n",
    "nickname = 'dll-cluster'\n",
    "\n",
    "#set whether or not you want to extract the fasta file for the whole region\n",
    "get_region = True\n",
    "\n",
    "#set the location of the fasta file you want to extract from\n",
    "fasta_file = '~/Labwork/Bioinformatics/GenomeSequences/Hya_azt/Hazt_2.0.1_genomic.fna'\n",
    "#set the destination directory for your output fasta files\n",
    "output_destination = '~/Labwork/Bioinformatics/ContigsandScaffolds/'\n",
    "\n",
    "####################################################\n",
    "####################################################\n",
    "\n",
    "#Decompose the IGV address into component parts\n",
    "contig = igv_address.split(':')[0]\n",
    "start = igv_address.split(':')[1].split('-')[0].replace(',', '')\n",
    "end = igv_address.split(':')[1].split('-')[1].replace(',', '')\n",
    "print('genomic region of interest is:', contig, start, end, '\\n')\n",
    "\n",
    "#Run this block to get the fasta file from your region of interest\n",
    "if get_region == True:\n",
    "    region = contig + '\\t' + start + '\\t' + end\n",
    "    region_name = contig + '_' + start + '_' + end \n",
    "    with open('region.bed', 'w+') as f:\n",
    "        f.write(region)\n",
    "    region_fasta = output_destination + nickname + '_' +  region_name + '.fasta'\n",
    "    region_bed = 'region.bed'\n",
    "    print('getting fasta file of coordinate', igv_address, 'and saving to:\\n', region_fasta, '\\n')\n",
    "    !bedtools getfasta -fi $fasta_file -bed $region_bed -fo $region_fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert VISTA to .gff\n",
    "1. Run [VISTA](http://genome.lbl.gov/cgi-bin/VistaInput?num_seqs=2)\n",
    "2. Modify correct file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_name = 'dll-cluster'\n",
    "\n",
    "Phaw_name = 'Par-haw_' + gene_name\n",
    "Hazt_name = 'Hya-azt_' + gene_name\n",
    "\n",
    "filename = 'VISTA_' + Phaw_name + '_vs_' + Hazt_name + '.txt'\n",
    "folder = '/Users/dennis/Labwork/Bioinformatics/VISTAAlignments/'\n",
    "\n",
    "vista_to_gff = pd.DataFrame()\n",
    "\n",
    "with open(folder + filename) as v:\n",
    "    lines = v.readlines()\n",
    "    Phaw_contigs = [line.lstrip('>' + Phaw_name).lstrip(' ').split(' ')[0] for line in lines if Phaw_name in line]\n",
    "    contig = [item.split(':')[0].split('_')[0] + '_' + item.split(':')[0].split('_')[1] for item in Phaw_contigs]\n",
    "    first = int(Phaw_contigs[0].split(':')[0].split('_')[2])\n",
    "    start = [int(item.split(':')[1].split('-')[0]) + first for item in Phaw_contigs]\n",
    "    stop = [int(item.split(':')[1].split('-')[1]) + first for item in Phaw_contigs]\n",
    "    Hazt_contigs = [line.lstrip('>' + Hazt_name + ' ').split(' ')[0] for line in lines if Hazt_name in line]\n",
    "    Orient = [line.lstrip('>' + Phaw_name + ' ').split(' ')[1].strip('()\\n') for line in lines if Phaw_name in line]\n",
    "    Stats = [line for line in lines if '%' in line]\n",
    "    Lens = [stat.split(',')[0].lstrip('= ').replace(' ', '') + ';' for stat in Stats]\n",
    "    Pid = [stat.split(',')[1].lstrip('identity = ').rstrip('%') for stat in Stats]\n",
    "    vista_to_gff['contig'], vista_to_gff['source'], vista_to_gff['feature'], vista_to_gff['start'], vista_to_gff['end'], vista_to_gff['score'], vista_to_gff['strand'], vista_to_gff['frame'], vista_to_gff['attribute'] = [contig, 'VISTA', Hazt_contigs, start, stop, Pid, Orient, '0', Lens]\n",
    "\n",
    "output = filename.replace('.txt', '.gff')\n",
    "vista_to_gff = vista_to_gff[['contig', 'source', 'feature', 'start', 'end', 'score', 'strand', 'frame', 'attribute']].drop_duplicates()\n",
    "vista_to_gff.to_csv(folder + output, header = None, sep = '\\t', index = None)\n",
    "vista_to_gff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect all VISTA .gff files and merge into a single .gff file for IGV\n",
    "\n",
    "For all VISTA alignments in the specified directory, this script will merge them all into a single .gff file. </br>\n",
    "This allows for updating of the .gff file in IGV without having to manually re-load the file. </br>\n",
    "After running this script, you may need to restart IGV for the changes to be reflected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of filenames\n",
    "directory = '/users/dennis/Labwork/Bioinformatics/VISTAAlignments/'\n",
    "filenames = [file for file in os.listdir(directory) if '.gff' in file]\n",
    "\n",
    "# Open file3 in write mode \n",
    "with open(directory + 'VISTA_peaks.gff', 'w+') as outfile: \n",
    "  \n",
    "    # Iterate through list \n",
    "    for name in filenames: \n",
    "        \n",
    "        if name == 'VISTA_peaks.gff':\n",
    "            continue\n",
    "  \n",
    "        # Open each file in read mode \n",
    "        with open(directory + name) as infile: \n",
    "  \n",
    "            # read the data from file1 and \n",
    "            # file2 and write it in file3 \n",
    "            outfile.write(infile.read()) \n",
    "  \n",
    "        # Add '\\n' to enter data of file2 \n",
    "        # from next line \n",
    "        outfile.write(\"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge all PWMs in gene_list into a single PWM file for FIMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwm_directory = '/users/dennis/Labwork/Bioinformatics/PWMs/'\n",
    "\n",
    "output_name = 'Dro-mel_Sp1_regulators.meme'\n",
    "gene_list = ['tll', 'Hb', 'dl']\n",
    "\n",
    "def common_member(a, b): \n",
    "    a_set = set(a) \n",
    "    b_set = set(b) \n",
    "    if (a_set & b_set): \n",
    "        return True \n",
    "    else: \n",
    "        return False\n",
    "\n",
    "match = False\n",
    "newfile = None\n",
    "files_added = []\n",
    "started = False\n",
    "    \n",
    "for file in sorted(os.listdir(pwm_directory)):\n",
    "    if 'meme' in file.split('.') and common_member(gene_list, file.rstrip('.meme').split('_')):\n",
    "        files_added += [file]\n",
    "        with open(pwm_directory + file) as file:            \n",
    "            for line in file:\n",
    "                if 'MOTIF' in line:\n",
    "                    match = True\n",
    "                    if started == False:\n",
    "                        newfile = open(pwm_directory + 'temp.meme', 'w+')\n",
    "                    else:\n",
    "                        newfile = open(pwm_directory + 'temp.meme', 'a+')\n",
    "                    newfile.write(line)\n",
    "                    continue\n",
    "                elif 'URL' in line:\n",
    "                    match = False\n",
    "                    newfile.write(line)\n",
    "                    newfile.write('\\n')\n",
    "                    newfile.close()\n",
    "                    continue\n",
    "                elif match:\n",
    "                    newfile.write(line)\n",
    "        started = True\n",
    "\n",
    "data = data2 = \"\" \n",
    "  \n",
    "with open(pwm_directory + 'MEME_header.txt') as fp: \n",
    "    data = fp.read() \n",
    "\n",
    "with open(pwm_directory + 'temp.meme') as fp: \n",
    "    data2 = fp.read() \n",
    "\n",
    "data += \"\\n\"\n",
    "data += data2\n",
    "  \n",
    "with open (pwm_directory + output_name, 'w+') as fp: \n",
    "    fp.write(data)\n",
    "\n",
    "print('Combined:')\n",
    "print([file for file in files_added])\n",
    "print('into one MEME file:', output_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert FIMO output into a .bed file \n",
    "If using a text-input PWM, set a value for custom_alias that will be the gene name used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '~/Labwork/Bioinformatics/FIMO/FIMO_Dro-mel_Pho_vs_Phaw-BXC_peaks.gff'\n",
    "custom_alias = ''\n",
    "\n",
    "fimo_output = pd.read_csv(input_file, sep = '\\t', skiprows = 1, header = None)\n",
    "fimo_output_name = input_file.replace('.gff', '_converted.bed')\n",
    "\n",
    "fimo_output['contig'] = fimo_output[0].str.split(\":\", n = 1, expand = True)[0]\n",
    "fimo_output['paststart'] = fimo_output[0].str.split(\":\", n = 1, expand = True)[1].str.split('-', n=1, expand = True)[0].astype(int)\n",
    "fimo_output['start'] = fimo_output[3].astype(int) + fimo_output['paststart']\n",
    "fimo_output['end'] = fimo_output[4].astype(int) + fimo_output['paststart']\n",
    "fimo_output['alias'] = fimo_output[8].str.split(';', n = 2, expand = True)[1].str.replace('Alias=', '')\n",
    "if custom_alias != '':\n",
    "    fimo_output['alias'] = custom_alias\n",
    "fimo_output['name'] = fimo_output[8].str.split(';', n = 1, expand = True)[0].str.lstrip('Name=')\n",
    "fimo_output['ID'] = fimo_output[8].str.split(';', n = 3, expand = True)[2].str.lstrip('ID=')\n",
    "fimo_output['other'] = fimo_output[8].str.split(';', n = 3, expand = True)[3].str.rstrip(';')\n",
    "fimo_output[5] = fimo_output[5] * 10\n",
    "\n",
    "fimo_output_export = fimo_output[['contig', 'start', 'end', 'alias', 5, 6]]\n",
    "fimo_output_export.to_csv(fimo_output_name, sep = '\\t', index = None, header = None)\n",
    "fimo_output_export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect all FIMO .bed files and merge into a single .bed file for IGV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of filenames\n",
    "directory = '/users/dennis/Labwork/Bioinformatics/FIMO/'\n",
    "filenames = [file for file in os.listdir(directory) if '_converted.bed' in file]\n",
    "\n",
    "# Open file3 in write mode \n",
    "with open(directory + 'FIMO_sites.bed', 'w+') as outfile: \n",
    "  \n",
    "    # Iterate through list \n",
    "    for name in filenames: \n",
    "  \n",
    "        # Open each file in read mode \n",
    "        with open(directory + name) as infile: \n",
    "  \n",
    "            # read the data from file1 and \n",
    "            # file2 and write it in file3 \n",
    "            outfile.write(infile.read()) \n",
    "  \n",
    "        # Add '\\n' to enter data of file2 \n",
    "        # from next line \n",
    "        outfile.write(\"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert RepeatMasker to .gff file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_rm = 'Par-haw_BXC'\n",
    "rm_alias = ''\n",
    "\n",
    "rm_loc = '~/Labwork/Bioinformatics/RepeatMasker/'\n",
    "rm_file_loc = '~/Labwork/Bioinformatics/RepeatMasker/' + input_rm + '_repeatmasker.txt'\n",
    "rm_bed_name = rm_file_loc.replace('.txt', '.bed')\n",
    "\n",
    "rm_output = pd.read_csv(rm_file_loc, sep = '\\t', header = None)\n",
    "\n",
    "rm_output['contig'] = rm_output[5].str.split(\":\", n = 1, expand = True)[0]\n",
    "rm_output['start'] = rm_output[5].str.split(\":\", n = 1, expand = True)[1].str.split('-', n = 1, expand = True)[0].astype('int') + rm_output[6]\n",
    "rm_output['end'] = rm_output[5].str.split(\":\", n = 1, expand = True)[1].str.split('-', n = 1, expand = True)[0].astype('int') + rm_output[7]\n",
    "rm_output['feature'] = rm_output[11] + '_' +  rm_output[10]\n",
    "\n",
    "rm_bed = rm_output[['contig', 'start', 'end', 'feature', 1, 9]]\n",
    "rm_bed.to_csv(rm_bed_name, sep = '\\t', index = None, header = None)\n",
    "display(rm_bed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect all RepeatMasker .bed files and merge into a single .bed file for IGV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of filenames\n",
    "directory = '/users/dennis/Labwork/Bioinformatics/RepeatMasker/'\n",
    "filenames = [file for file in os.listdir(directory) if '_repeatmasker.bed' in file]\n",
    "rm_collated = directory + 'Repeatmasker_sites.bed'\n",
    "rm_sorted = rm_collated.replace('.bed', '.sorted.bed')\n",
    "\n",
    "# Open file3 in write mode \n",
    "with open(rm_collated, 'w+') as outfile: \n",
    "  \n",
    "    # Iterate through list \n",
    "    for name in filenames: \n",
    "  \n",
    "        # Open each file in read mode \n",
    "        with open(directory + name) as infile: \n",
    "  \n",
    "            # read the data from file1 and \n",
    "            # file2 and write it in file3 \n",
    "            outfile.write(infile.read()) \n",
    "  \n",
    "        # Add '\\n' to enter data of file2 \n",
    "        # from next line \n",
    "        outfile.write(\"\\n\")\n",
    "\n",
    "!bedtools sort -i $rm_collated > $rm_sorted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
