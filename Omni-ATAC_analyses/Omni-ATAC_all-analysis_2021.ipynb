{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATAC-Seq Analysis for Omni-ATAC-Seq\n",
    "This script starts from raw reads and performs all analyses necessary, with a final output of FastQC files, trimmed and unpaired reads, Genrich peaks, and BedGraph files.\n",
    "\n",
    "__Required software:__ trim_galore, cutadapt, FastQC, samtools, bedtools, java, igvtools, bedGraphToBigWig, Genrich\n",
    "\n",
    "The script expects the following input data:\n",
    "1. Untrimmed .fastq.gz files in the same folder as the script.\n",
    "2. A .fasta file containing the contigs that will be used for the analysis.\n",
    "3. A bowtie2 library made using bowtie2-build.\n",
    "4. A chrom_sizes file made using samtools and cut (see section 0.5)\n",
    "\n",
    "The script performs the following steps:\n",
    "1. Trims reads using __trim_galore__, which both trims and performs FastQC analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#To convert this to a Python script, use:\n",
    "#jupyter nbconvert --to script Omni-ATAC_all-analysis.ipynb\n",
    "\n",
    "#To run on a SLURM scheduler or other shared computing system, you may need to load modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Configuration\n",
    "\n",
    "This section loads in required Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuration of Python script\n",
    "import subprocess\n",
    "import os\n",
    "import os.path\n",
    "import fnmatch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.5. Utilities\n",
    "\n",
    "This section contains utilities for generating necessary downstream files.\n",
    "\n",
    "__Required software:__ samtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "### Fill in this information before running this section ####\n",
    "\n",
    "#Set the names of your .fasta file\n",
    "genome_file = 'phaw_5.0.fa'\n",
    "\n",
    "#Decide if you want to generatea .chrom.sizes file\n",
    "make_chromsizes = False\n",
    "\n",
    "#############################################################\n",
    "#############################################################\n",
    "\n",
    "if make_chromsizes:\n",
    "    genome_index = genome_file.replace('.fa', '.fa.fai')\n",
    "    chromsizes = genome_file.replace('.fa', '.chrom.sizes')\n",
    "    \n",
    "    !samtools faidx {genome_file}\n",
    "    !cut -f1,2 {genome_index} > {chromsizes}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Trim-Galore\n",
    "Run this portion of the script in a folder containing the raw Omni-ATAC reads.\n",
    "\n",
    "Omni-ATAC data was sequenced on two different lanes of Illumina NovaSeq. </br>\n",
    "Reads from the first run contain the common string \"L001\", while reads from the second run contain the common string \"L002\".\n",
    "\n",
    "__Required software:__ trim_galore, cutadapt, FastQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#############################################################\n",
    "### Fill in this information before running this section ####\n",
    "\n",
    "#Decide if you want to run this section\n",
    "run_read_trimming_run1 = False\n",
    "run_read_trimming_run2 = False\n",
    "\n",
    "#Write a base pattern for your raw fastq file names\n",
    "#This pattern should occur in all of your file names\n",
    "base_pattern_run1 = 'S*L001_R1_001.fastq.gz'\n",
    "base_pattern_run2 = 'S*L002_R1_001.fastq.gz'\n",
    "\n",
    "#############################################################\n",
    "#############################################################\n",
    "\n",
    "#checks if you want to trim your reads\n",
    "if run_read_trimming_run1 == True:\n",
    "\n",
    "    #Scan through all of the files in the folder specified within the single quotes in the line below\n",
    "    #for each file listed in a sorted list of the directory's contents\n",
    "    for file in sorted(os.listdir('.')):\n",
    "        #determines if the file is a .fastq.gz file\n",
    "        if fnmatch.fnmatch(file, base_pattern_run1):\n",
    "            #gets the name of that file\n",
    "            one = file\n",
    "            #simulates the name of the other read of that file\n",
    "            two = file.replace('R1_001','R2_001')\n",
    "            #checks if there is not an appropriate pair for the file, if so, then exits and returns an error\n",
    "            if not os.path.exists(two):\n",
    "                raise Exception('A matching paired file for', file, 'was not found')\n",
    "            #sends progress message to stdout\n",
    "            !echo 'Performing trim-galore on' $one 'and' $two\n",
    "            #performs trim_galore on the pair of reads identified, removing Nextera adapters and performing FastQC\n",
    "            !trim_galore --fastqc --paired --nextera --stringency 5 --gzip --retain_unpaired $one $two\n",
    "    !mv *_val_* ../2021_reanalysis/\n",
    "    !mv *_unpaired_* ../2021_reanalysis/\n",
    "\n",
    "if run_read_trimming_run2 == True:\n",
    "    #Scan through all of the files in the folder specified within the single quotes in the line below\n",
    "    #for each file listed in a sorted list of the directory's contents\n",
    "    for file in sorted(os.listdir('.')):\n",
    "        #determines if the file is a .fastq.gz file\n",
    "        if fnmatch.fnmatch(file, base_pattern_run2):\n",
    "            #gets the name of that file\n",
    "            one = file\n",
    "            #simulates the name of the other read of that file\n",
    "            two = file.replace('R1_001','R2_001')\n",
    "            #checks if there is not an appropriate pair for the file, if so, then exits and returns an error\n",
    "            if not os.path.exists(two):\n",
    "                raise Exception('A matching paired file for', file, 'was not found')\n",
    "            #sends progress message to stdout\n",
    "            !echo 'Performing trim-galore on' $one 'and' $two\n",
    "            #performs trim_galore on the pair of reads identified, removing Nextera adapters and performing FastQC\n",
    "            !trim_galore --fastqc --paired --nextera --stringency 5 --gzip --retain_unpaired $one $two\n",
    "    !mv *_val_* ../2021_reanalysis/\n",
    "    !mv *_unpaired_* ../2021_reanalysis/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bowtie2 Alignment and MACS2 Peak Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#############################################################\n",
    "### Fill in this information before running this section ####\n",
    "\n",
    "#Decide if you want to run this section\n",
    "run_bowtie2_align = False\n",
    "run_chrM_align = False\n",
    "\n",
    "#Set the names of your .fasta file\n",
    "genome_file = 'phaw_5.0.fa'\n",
    "\n",
    "#Container for the chromsizes variable\n",
    "bt2lib = genome_file.replace('.fa', '.bt2lib')\n",
    "bt2libmarker = genome_file.replace('.fa', '.bt_marker.txt')\n",
    "bt2marker_exists = False\n",
    "chromsizes = genome_file.replace('.fa', '.chrom.sizes')\n",
    "\n",
    "#Specify the path of your tool locations for Picard and igvtools\n",
    "picardloc = '/clusterfs/vector/home/groups/software/sl-7.x86_64/modules/picard/2.9.0/lib/picard.jar'\n",
    "igvtoolsloc = '/clusterfs/vector/home/groups/software/sl-7.x86_64/modules/igvtools/2.3.98/igvtools'\n",
    "bgtobwloc = '/global/scratch/dasun/bedGraphToBigWig'\n",
    "\n",
    "#Set the desired q-score of your read alignments\n",
    "qscore = 10\n",
    "\n",
    "#############################################################\n",
    "#############################################################\n",
    "\n",
    "#checks if you want to trim your reads\n",
    "if run_bowtie2_align == True:\n",
    "\n",
    "    #check if the bt2 marker has already been flagged\n",
    "    !printf 'checking if index file has already been generated for '$genome_file'\\n'\n",
    "    for file in sorted(os.listdir('.')):\n",
    "        if fnmatch.fnmatch(file, bt2libmarker):\n",
    "            bt2marker_exists = True\n",
    "            !printf 'genome index file already exists for '$genome_file'\\n'\n",
    "\n",
    "    #search through current directory\n",
    "    for file in sorted(os.listdir('.')):\n",
    "        #if the genome file is found and there is no bt2marker_exists flag\n",
    "        if fnmatch.fnmatch(file, genome_file) and bt2marker_exists == False:\n",
    "            #check for files in the current directory\n",
    "            !printf 'no index file exists for '$genome_file'\\n'\n",
    "            !printf 'generating bt2 index for '$genome_file'\\n'\n",
    "            bt2_output = genome_file.replace('.fa', '_bowtie2-build-output.txt')\n",
    "            !bowtie2-build -f --threads 12 $genome_file $bt2lib > $bt2_output\n",
    "        \n",
    "            #generate chrom.sizes file\n",
    "            fainame = genome_file + '.fai'\n",
    "            !samtools faidx $genome_file\n",
    "            !cut -f1,2 $fainame > $chromsizes\n",
    "\n",
    "            #make a marker file to indicate that the bowtie2 library has already been built\n",
    "            !touch $bt2libmarker\n",
    "\n",
    "    #check for files in the current directory\n",
    "    for file in sorted(os.listdir('.')):\n",
    "    \n",
    "        #initialize and/or blank variables\n",
    "        one = ''\n",
    "        two = ''\n",
    "        unone = ''\n",
    "        untwo = ''\n",
    "        prefix = ''\n",
    "    \n",
    "        #determine the file name and its partners, if it is R1 trimmed read\n",
    "        if fnmatch.fnmatch(file,'S*L00*val_1*.fq.gz'):\n",
    "            one = file\n",
    "            two = file.replace('R1_001_val_1','R2_001_val_2')\n",
    "            unone = one.replace('val','unpaired')\n",
    "            untwo = two.replace('val','unpaired')\n",
    "            !printf 'starting analysis on '$one' '$two' '$unone' '$untwo'\\n'\n",
    "        else: continue\n",
    "    \n",
    "        #determine the prefix of the file and the run\n",
    "        if fnmatch.fnmatch(file, '*L001*'):\n",
    "            prefix = file.partition(\"_\")[0]+'_run1'\n",
    "            !printf 'prefix is '$prefix'\\n'\n",
    "        elif fnmatch.fnmatch(file, '*L002*'):\n",
    "            prefix = file.partition(\"_\")[0]+'_run2'\n",
    "            !printf 'prefix is '$prefix'\\n'\n",
    "        else: continue\n",
    "        \n",
    "        #initialize variables from dictionaries\n",
    "        !printf 'starting analysis of '$prefix'\\n'\n",
    "    \n",
    "        #name sam file\n",
    "        samname = prefix + '.sam'\n",
    "    \n",
    "        #run bowtie2\n",
    "        !printf 'running bowtie2 on '$prefix'\\n'\n",
    "        !bowtie2 --local --very-sensitive-local --threads 40 --time -x $bt2lib -1 $one -2 $two -U $unone,$untwo -S $samname\n",
    "\n",
    "        #name bam files\n",
    "        bampref = samname.replace('.sam','_sorted')\n",
    "        bamname = samname.replace('.sam','_q' + str(qscore) + '.bam')\n",
    "        !printf 'running samtools sort on '$prefix'\\n'\n",
    "        !samtools view -bS -q $qscore $samname | samtools sort -T $bampref -o $bamname\n",
    "    \n",
    "        #deduplicate bam files\n",
    "        dedupname = bamname.replace('.bam','.dedup.bam')\n",
    "        picardstats = bamname.replace('.bam','.txt')\n",
    "        !printf 'running picardtools on '$prefix'\\n'\n",
    "        !java -jar $picardloc MarkDuplicates REMOVE_DUPLICATES=TRUE I=$bamname O=$dedupname M=$picardstats\n",
    "    \n",
    "        #shift files to match predicted Tn5 insertion sites\n",
    "        shiftedname = bamname.replace('.bam','.shifted.bed')\n",
    "        !printf 'shifting reads for '$prefix'\\n'\n",
    "        awkcommand = \"\\'BEGIN {OFS = \\\"\\t\\\"} ; {if ($6 == \\\"+\\\") print $1, $2 + 4, $3 + 4, $4, $5, $6; else {if($2 - 5 > 0) print $1, $2 - 5, $3 - 5, $4, $5, $6}}\\'\"\n",
    "        !bedtools bamtobed -i $dedupname | awk $awkcommand > $shiftedname\n",
    "\n",
    "        #make bedgraph file\n",
    "        bgname = bamname.replace('.bam','.bedgraph')\n",
    "        !printf 'making bedgraph file for '$prefix'\\n'\n",
    "        !bedtools genomecov -i $shiftedname -g $chromsizes -bg > $bgname\n",
    "\n",
    "        #make indexed file\n",
    "        !printf 'making index file of '$prefix'\\n'\n",
    "        !$igvtoolsloc index $dedupname\n",
    "    \n",
    "        #convert bedgraph to bigwig\n",
    "        !printf 'converting '$bgname' to bigwig'\n",
    "        bwname = bgname.replace('.bedgraph', '.bw')\n",
    "        !$bgtobwloc $bgname $chromsizes $bwname\n",
    "        !printf '*** analysis of '$prefix' completed ***\\n \\n'\n",
    "\n",
    "        #checks if you want to run chrMalign\n",
    "        if run_chrM_align == True:\n",
    "            chrMsamname = prefix+'_chrMalign.sam'\n",
    "            chrMoutputfile = prefix+'_chrMalign.txt'\n",
    "            !printf 'analyzing mtDNA content of '$prefix'\\n'\n",
    "            !bowtie2 --local --very-sensitive-local --threads 40 --time -x par.haw.chrM.bt2 -1 $one -2 $two -U $unone,$untwo -S $chrMsamname >>$chrMoutputfile 2>&1\n",
    "            !printf '*** analysis of '$prefix' mtDNA contamination completed! yay! ***\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Run Genrich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#############################################################\n",
    "### Fill in this information before running this section ####\n",
    "\n",
    "#Decide what steps you want to do\n",
    "run_merge_bam = False\n",
    "run_Genrich = False\n",
    "run_Genrich_ATACmodeonly = False\n",
    "\n",
    "#The settings used for the final analysis are included in this run of Genrich\n",
    "run_Genrich_ATACrerun = False\n",
    "\n",
    "#Write a dictionary with prefixes and conditions\n",
    "base_pattern_2 = 'S*_*.bam'\n",
    "\n",
    "#Specify location of Genrich\n",
    "genrichloc = '/global/scratch/dasun/ATAC-Seq_phaw5.0/Genrich/Genrich'\n",
    "\n",
    "#############################################################\n",
    "#############################################################\n",
    "\n",
    "#checks if you want to merge the bam files\n",
    "if run_merge_bam == True:\n",
    "    \n",
    "    #makes a temporary dictionary to hold the files\n",
    "    temp_dict = {}\n",
    "    \n",
    "    #Goes through all of the files and groups them into temp_dict\n",
    "    for file in sorted(os.listdir('.')):\n",
    "        if fnmatch.fnmatch(file, base_pattern_2) and 'dedup' not in file.split('.'):\n",
    "            prefix = file.split('_')[0]\n",
    "            if prefix not in temp_dict:\n",
    "                temp_dict[prefix] = [file]\n",
    "                !printf 'placed '$file' in '$prefix' bin in temp_dict\\n'\n",
    "            else:\n",
    "                temp_dict[prefix] += [file]\n",
    "                !printf 'placed '$file' in '$prefix' bin in temp_dict\\n'\n",
    "    \n",
    "    !printf 'temp_dict contains '$temp_dict'\\n'\n",
    "    \n",
    "    #For each condition (replicate)\n",
    "    for pref in temp_dict:\n",
    "        !printf 'checking runs for'$pref'\\n'\n",
    "        if len(temp_dict[pref]) > 1:\n",
    "            !printf 'multiple runs detected for '$pref'\\n'\n",
    "            genrich_bam = temp_dict[pref][0].replace('.bam', '.Genrich.bam').replace('run1', 'bothruns')\n",
    "            genrich_sorted = genrich_bam.replace('.Genrich', '.Genrich_sorted')\n",
    "            file1 = temp_dict[pref][0]\n",
    "            file2 = temp_dict[pref][1]\n",
    "            !printf 'now merging '$file1' '$file2' into '$genrich_bam'\\n'\n",
    "            !samtools merge $genrich_bam $file1 $file2\n",
    "            !printf 'now sorting '$genrich_bam' into '$genrich_sorted'\\n'\n",
    "            !samtools sort -n -o $genrich_sorted $genrich_bam\n",
    "        else:\n",
    "            !printf 'only one run detected for '$pref'\\n'\n",
    "            genrich_bam = temp_dict[pref][0].replace('.bam', '.Genrich.bam')\n",
    "            genrich_sorted = genrich_bam.replace('.Genrich', '.Genrich_sorted')\n",
    "            file1 = temp_dict[pref][0]\n",
    "            !printf 'now sorting '$file1' into '$genrich_sorted'\\n'\n",
    "            !samtools sort -n -o $genrich_sorted $file1\n",
    "\n",
    "#checks if you want to run Genrich\n",
    "if run_Genrich == True:\n",
    "    for file in sorted(os.listdir('.')):\n",
    "        if 'Genrich_sorted' in file.split('.') and 'A' in list(str(file)):\n",
    "            other_rep = file.replace('A', 'B')\n",
    "            !printf 'now analyzing '$file' and '$other_rep'\\n'\n",
    "            narrowPeak = file.replace('.bam', '.narrowPeak')\n",
    "            pq = file.replace('.bam', '.pq.bedgraph')\n",
    "            pileup = file.replace('.bam', '.p.pileup.bedgraph')\n",
    "            atac_mode = file.replace('.bam', '.ATAC.narrowPeak')\n",
    "            !printf 'now performing Genrich on '$file' and '$other_rep', output to '$narrowPeak'\\n'\n",
    "            !$genrichloc -t $file,$other_rep -o $narrowPeak -f $pq -k $pileup -rv\n",
    "            !printf 'now performing Genrich ATAC-mode on '$file' and '$other_rep', output to '$atac_mode'\\n'\n",
    "            !$genrichloc -t $file,$other_rep -o $atac_mode -rv\n",
    "\n",
    "#running Genrich in ATAC mode only\n",
    "if run_Genrich_ATACmodeonly == True:\n",
    "    for file in sorted(os.listdir('.')):\n",
    "        if 'Genrich_sorted' in file.split('.') and not 'sam' in file.split('.') and 'A' in list(str(file)):\n",
    "            other_rep = file.replace('A', 'B')\n",
    "            !printf 'now analyzing '$file' and '$other_rep'\\n'\n",
    "            pq = file.replace('.bam', '.ATAC.pq.bedgraph')\n",
    "            pileup = file.replace('.bam', '.ATAC.p.pileup.bedgraph')\n",
    "            atac_mode = file.replace('.bam', '.ATAC.narrowPeak')\n",
    "            !printf 'now performing Genrich ATAC-mode on '$file' and '$other_rep', output to '$atac_mode'\\n'\n",
    "            !$genrichloc -t $file,$other_rep -o $atac_mode -f $pq -k $pileup -rv\n",
    "            \n",
    "#running Genrich in ATAC mode only\n",
    "if run_Genrich_ATACrerun == True:\n",
    "    for file in sorted(os.listdir('.')):\n",
    "        if '.Genrich_sorted.bam' in file.split('_q10') and 'A' in list(str(file)):\n",
    "            other_rep = file.replace('A_', 'B_')\n",
    "            !printf 'now analyzing '$file' and '$other_rep'\\n'\n",
    "            pq = file.replace('.bam', '.ATAC.q005.bedgraph')\n",
    "            pileup = file.replace('.bam', '.ATAC.q005.pileup.bedgraph')\n",
    "            atac_mode = file.replace('.bam', '.ATAC.q005.narrowPeak')\n",
    "            !printf 'now performing Genrich ATAC-mode on '$file' and '$other_rep', output to '$atac_mode'\\n'\n",
    "            !$genrichloc -t $file,$other_rep -o $atac_mode -f $pq -k $pileup -rvq 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Process Genrich Output Files\n",
    "\n",
    "Genrich generates a single bedgraph-like file that contains the bedgraph pileups from each replicate, concatenated. </br>\n",
    "The bash script below splits the bedgraph-like file into two files per stage, one for each replicate.\n",
    "\n",
    "#### Run this bash script:\n",
    "`for f in S*.q005.pileup.bedgraph* ; do \\\n",
    "    printf 'splitting '$f'\\n'`\n",
    "    awk '/experimental file/{filename=NR\"__'$f'\"}; {print >filename}' $f\n",
    "done`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "### Fill in this information before running this section ####\n",
    "\n",
    "#Decide what steps you want to do\n",
    "run_rename_Genrich = False\n",
    "run_rename_Genrich_again = False\n",
    "run_make_bigwig = False\n",
    "\n",
    "#Write a dictionary with prefixes and conditions\n",
    "base_pattern_3 = 'S*.ATAC.q005.pileup.bedgraph'\n",
    "\n",
    "#############################################################\n",
    "#############################################################\n",
    "\n",
    "if run_rename_Genrich == True:\n",
    "    #split and rename the file matching the base_pattern_3\n",
    "    sample_files = {}\n",
    "    \n",
    "    for file in sorted(os.listdir('.')):\n",
    "        if 'line' in file.split('_'):\n",
    "            stage = file.split('_')[2]\n",
    "            if stage in sample_files:\n",
    "                sample_files[stage] += [file]\n",
    "                !printf 'placed '$file' in '$stage' bin in temp_dict\\n'\n",
    "            elif stage not in sample_files:\n",
    "                sample_files[stage] = [file]\n",
    "                !printf 'placed '$file' in '$stage' bin in temp_dict\\n'\n",
    "    \n",
    "    for stage in sample_files:\n",
    "        A_sample = 'a'\n",
    "        B_sample = 'b'\n",
    "        if int(sample_files[stage][0].split('_')[0]) < int(sample_files[stage][1].split('_')[0]):\n",
    "            A_sample = sample_files[stage][0]\n",
    "            B_sample = sample_files[stage][1]\n",
    "            !printf 'A replicate is '$A_sample' and B replicate is '$B_sample'\\n'\n",
    "        elif int(sample_files[stage][1].split('_')[0]) < int(sample_files[stage][0].split('_')[0]):\n",
    "            A_sample = sample_files[stage][1]\n",
    "            B_sample = sample_files[stage][0]\n",
    "            !printf 'A replicate is '$A_sample' and B replicate is '$B_sample'\\n'\n",
    "        if A_sample == 'a' or B_sample == 'b':\n",
    "            raise Exception('Parsing Error!')\n",
    "        new_A1 = A_sample.replace('.q005.pileup.bedgraph', '.q005.pileup.split.bedgraph')\n",
    "        new_A = new_A1.lstrip('_line_')\n",
    "        new_B = new_A.replace('A_bothruns', 'B_bothruns')\n",
    "        !printf 'renaming '$A_sample' to '$new_A'\\n'\n",
    "        !mv -n $A_sample $new_A\n",
    "        !printf 'renaming '$B_sample' to '$new_B'\\n'\n",
    "        !mv -n $B_sample $new_B\n",
    "\n",
    "if run_rename_Genrich_again == True:\n",
    "    for file in sorted(os.listdir('.')):\n",
    "        if 'line' in file.split('_'):\n",
    "            newfilename = file.replace('q005.pileup.bedgraph', '.q005.pileup.split.bedgraph')\n",
    "            newfilename2 = newfilename.lstrip('_line_')\n",
    "            !mv -n $file $newfilename2\n",
    "\n",
    "if run_make_bigwig == True:\n",
    "    for file in sorted(os.listdir('.')):\n",
    "        if 'split' in file.split('.'):\n",
    "            newfile = file.replace('split', 'headless')\n",
    "            !printf 'removingheader of '$file' and making '$newfile'\\n'\n",
    "            !sed '1,2d' $file > $newfile\n",
    "            noctrlfile = newfile.replace('headless', 'noctrl')\n",
    "            !printf 'REMOVING ctrl column from '$newfile' to '$noctrlfile'\\n'\n",
    "            !cut -f1,2,3,4 $newfile > $noctrlfile\n",
    "            bigwigfile = noctrlfile.replace('.bedgraph', '.bw')\n",
    "            !printf 'STARTING converting '$noctrlfile' to '$bigwigfile'\\n'\n",
    "            !./bedGraphToBigWig $noctrlfile phaw_5.0.chrom.sizes $bigwigfile\n",
    "            !printf 'FINISHED converting '$noctrlfile' to '$bigwigfile'\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Process Genrich Output Files\n",
    "\n",
    "This section generates a merged peak file across all timepoints by using bedtools merge. </br>\n",
    "The version of the script below also generates a list of which peak numbers from each stage-specific peak file were merged into the merged peaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joining narrowPeak files: Genrich_sorted_bothruns/S13A_bothruns_q10.Genrich_sorted.ATAC.q005.named.narrowPeak"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>phaw_50.000003</td>\n",
       "      <td>4302</td>\n",
       "      <td>5642</td>\n",
       "      <td>15</td>\n",
       "      <td>S27_0,S15_0,S23_0,S19_0,S17_0,S14_0,S25_0,S18_...</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>2054.581486</td>\n",
       "      <td>5.871578</td>\n",
       "      <td>7.869445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>phaw_50.000009</td>\n",
       "      <td>11628</td>\n",
       "      <td>11888</td>\n",
       "      <td>1</td>\n",
       "      <td>S26_1</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>269.657501</td>\n",
       "      <td>2.950491</td>\n",
       "      <td>2.950491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>phaw_50.000014</td>\n",
       "      <td>206406</td>\n",
       "      <td>207519</td>\n",
       "      <td>15</td>\n",
       "      <td>S17_1,S13_1,S19plus_1,S25_1,S15_1,S20_1,S18_1,...</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1376.176314</td>\n",
       "      <td>4.676017</td>\n",
       "      <td>7.024645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>phaw_50.000014</td>\n",
       "      <td>238984</td>\n",
       "      <td>239410</td>\n",
       "      <td>3</td>\n",
       "      <td>S27_2,S20_2,S21_2</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>403.039642</td>\n",
       "      <td>3.178571</td>\n",
       "      <td>4.100616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>phaw_50.000014</td>\n",
       "      <td>239711</td>\n",
       "      <td>240244</td>\n",
       "      <td>12</td>\n",
       "      <td>S19plus_2,S19_2,S25_2,S23_2,S24_2,S27_3,S17_2,...</td>\n",
       "      <td>944.500000</td>\n",
       "      <td>766.406303</td>\n",
       "      <td>2.249521</td>\n",
       "      <td>5.012049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190073</th>\n",
       "      <td>phaw_50.283875b</td>\n",
       "      <td>1535466</td>\n",
       "      <td>1535680</td>\n",
       "      <td>2</td>\n",
       "      <td>S27_100575,S26_103598</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>223.467064</td>\n",
       "      <td>3.383217</td>\n",
       "      <td>3.640883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190074</th>\n",
       "      <td>phaw_50.283875b</td>\n",
       "      <td>1546841</td>\n",
       "      <td>1547714</td>\n",
       "      <td>13</td>\n",
       "      <td>S23_108489,S25_100504,S24_106236,S20_97667,S27...</td>\n",
       "      <td>926.461538</td>\n",
       "      <td>1193.281705</td>\n",
       "      <td>2.655820</td>\n",
       "      <td>8.145149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190075</th>\n",
       "      <td>phaw_50.283875b</td>\n",
       "      <td>1548468</td>\n",
       "      <td>1548807</td>\n",
       "      <td>1</td>\n",
       "      <td>S21_114857</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>384.672821</td>\n",
       "      <td>3.372366</td>\n",
       "      <td>3.372366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190076</th>\n",
       "      <td>phaw_50.283876</td>\n",
       "      <td>1905</td>\n",
       "      <td>3313</td>\n",
       "      <td>15</td>\n",
       "      <td>S15_63848,S17_73305,S13_65799,S19_84476,S22_99...</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>3321.552816</td>\n",
       "      <td>6.172129</td>\n",
       "      <td>7.825217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190077</th>\n",
       "      <td>phaw_50.283876</td>\n",
       "      <td>24624</td>\n",
       "      <td>25204</td>\n",
       "      <td>15</td>\n",
       "      <td>S20_97669,S17_73306,S13_65800,S19_84477,S19plu...</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1297.814168</td>\n",
       "      <td>5.064723</td>\n",
       "      <td>6.935948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190078 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0        1        2   3  \\\n",
       "0        phaw_50.000003     4302     5642  15   \n",
       "1        phaw_50.000009    11628    11888   1   \n",
       "2        phaw_50.000014   206406   207519  15   \n",
       "3        phaw_50.000014   238984   239410   3   \n",
       "4        phaw_50.000014   239711   240244  12   \n",
       "...                 ...      ...      ...  ..   \n",
       "190073  phaw_50.283875b  1535466  1535680   2   \n",
       "190074  phaw_50.283875b  1546841  1547714  13   \n",
       "190075  phaw_50.283875b  1548468  1548807   1   \n",
       "190076   phaw_50.283876     1905     3313  15   \n",
       "190077   phaw_50.283876    24624    25204  15   \n",
       "\n",
       "                                                        4            5  \\\n",
       "0       S27_0,S15_0,S23_0,S19_0,S17_0,S14_0,S25_0,S18_...  1000.000000   \n",
       "1                                                   S26_1  1000.000000   \n",
       "2       S17_1,S13_1,S19plus_1,S25_1,S15_1,S20_1,S18_1,...  1000.000000   \n",
       "3                                       S27_2,S20_2,S21_2  1000.000000   \n",
       "4       S19plus_2,S19_2,S25_2,S23_2,S24_2,S27_3,S17_2,...   944.500000   \n",
       "...                                                   ...          ...   \n",
       "190073                              S27_100575,S26_103598  1000.000000   \n",
       "190074  S23_108489,S25_100504,S24_106236,S20_97667,S27...   926.461538   \n",
       "190075                                         S21_114857  1000.000000   \n",
       "190076  S15_63848,S17_73305,S13_65799,S19_84476,S22_99...  1000.000000   \n",
       "190077  S20_97669,S17_73306,S13_65800,S19_84477,S19plu...  1000.000000   \n",
       "\n",
       "                  6         7         8  \n",
       "0       2054.581486  5.871578  7.869445  \n",
       "1        269.657501  2.950491  2.950491  \n",
       "2       1376.176314  4.676017  7.024645  \n",
       "3        403.039642  3.178571  4.100616  \n",
       "4        766.406303  2.249521  5.012049  \n",
       "...             ...       ...       ...  \n",
       "190073   223.467064  3.383217  3.640883  \n",
       "190074  1193.281705  2.655820  8.145149  \n",
       "190075   384.672821  3.372366  3.372366  \n",
       "190076  3321.552816  6.172129  7.825217  \n",
       "190077  1297.814168  5.064723  6.935948  \n",
       "\n",
       "[190078 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#############################################################\n",
    "### Fill in this information before running this section ####\n",
    "\n",
    "#Decide what steps you want to do\n",
    "run_narrowPeak_name = False\n",
    "run_narrowPeak_merge = False\n",
    "\n",
    "#Write a dictionary with prefixes and conditions\n",
    "narrowPeakdir = 'Genrich_sorted_bothruns/'\n",
    "narrowPeakpattern = 'A_bothruns_q10.Genrich_sorted.ATAC.q005.narrowPeak'\n",
    "namedPeakpattern = 'A_bothruns_q10.Genrich_sorted.ATAC.q005.named.narrowPeak'\n",
    "catname = 'Scat_bothruns_q10.Genrich_sorted.ATAC.q005.named.narrowPeak'\n",
    "sortedcatname = 'Scat_bothruns_q10.Genrich_sorted.ATAC.q005.sorted.narrowPeak'\n",
    "mergedname = 'Sall_bothruns_q10.Genrich_sorted.ATAC.q005.merged.narrowPeak'\n",
    "bampattern = 'AB_bothruns_q10.Genrich_resorted.bam'\n",
    "\n",
    "#############################################################\n",
    "#############################################################\n",
    "stages = ['S13', 'S14', 'S15', 'S17', 'S18', 'S19', 'S19plus', 'S20', 'S21', 'S22', 'S23', 'S24', 'S25', 'S26', 'S27']\n",
    "\n",
    "#names peaks from each peakfile based on stage\n",
    "if run_narrowPeak_name == True:\n",
    "    \n",
    "    #generate list of file names based on pattern and stage and directory\n",
    "    narrowPeak_file_list = [narrowPeakdir + i + narrowPeakpattern for i in stages]\n",
    "    \n",
    "    for file in narrowPeak_file_list:\n",
    "        #extract prefix from narrowPeak file name\n",
    "        prefix = file.replace(narrowPeakdir, '').replace(narrowPeakpattern, '')\n",
    "        \n",
    "        #load in narrowPeak file\n",
    "        narrowPeakdf = pd.DataFrame(pd.read_csv(file, sep = '\\t', header = None))\n",
    "        \n",
    "        #change peak name to reflect developmental stage\n",
    "        narrowPeakdf[3] = narrowPeakdf[3].str.split('_', expand = True)[1]\n",
    "        narrowPeakdf[3] = prefix + '_' + narrowPeakdf[3].astype('str')\n",
    "        \n",
    "        #save to a new file\n",
    "        newfile = file.replace('.narrowPeak', '.named.narrowPeak')\n",
    "        narrowPeakdf.to_csv(newfile, sep = '\\t', index = None, header = None)\n",
    "\n",
    "if run_narrowPeak_merge == True:\n",
    "    #generate list of file names based on pattern and stage and directory\n",
    "    narrowPeak_file_list = [narrowPeakdir + i + namedPeakpattern for i in stages]\n",
    "    \n",
    "    #concatenate into string and report string\n",
    "    narrowPeak_files = ' '.join(narrowPeak_file_list)\n",
    "    !printf 'joining narrowPeak files: '{narrowPeak_files}'\\n'\n",
    "    \n",
    "    #merge all into a new file with particular specifications\n",
    "    #note that this new file will be located in the parent directory of all the others\n",
    "    #standard narrowPeak format has these headers (using 1-index)\n",
    "    #1: chrom, 2: chromStart, 3: chromEnd, 4: name, \n",
    "    #5: score, 6: strand, 7: signalValue, 8: pValue, 9: qValue, 10: peak\n",
    "    #settings:\n",
    "    #-count all names\n",
    "    #-collapse peak names into list\n",
    "    #-get mean score\n",
    "    #-get mean signalValue\n",
    "    #-get min q value\n",
    "    #-get max q value\n",
    "    !cat {narrowPeak_files} > {catname}\n",
    "    !/usr/local/bin/bedtools sort -i {catname} > {sortedcatname}\n",
    "    !/usr/local/bin/bedtools merge -c 4,4,5,7,9,9 -o count,collapse,mean,mean,min,max -i {sortedcatname} > {mergedname}\n",
    "    \n",
    "    mergeddf = pd.DataFrame(pd.read_csv(mergedname, sep = '\\t', header = None))\n",
    "    display(mergeddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
